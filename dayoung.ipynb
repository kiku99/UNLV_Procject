{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bf7b6f-cc83-498a-a2a0-0d5684fe0cee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22012d73-1b85-4293-82b8-cc743dd8516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bcf7ad5-ed89-4cde-a6d4-1303e685eda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding-Done.\n",
      "Down Sampling-Done.\n",
      "Scaling-Done.\n",
      "- Start: training\n",
      "- End: training\n",
      "\n",
      "AUC = 0.531\n",
      "Elapsed Time: 1.0 min 4.22 sec\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.drop([\"Id\"], axis=1, inplace=True)\n",
    "\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df.rename(columns={\"married/single\": \"married_single\"}, inplace=True)\n",
    "\n",
    "# Category cols to num\n",
    "cate_cols = [\"married_single\", \"profession\", \"house_ownership\", \"car_ownership\", \"city\", \"state\"]\n",
    "\n",
    "for col in cate_cols:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(df[col])\n",
    "    df[col] = le.transform(df[col])\n",
    "\n",
    "print(\"Label Encoding-Done.\")\n",
    "\n",
    "# Down sampling\n",
    "subset_0 = df[df[\"risk_flag\"] == 0]\n",
    "subset_1 = df[df[\"risk_flag\"] == 1]\n",
    "\n",
    "subset_0_downsampled = resample(subset_0,\n",
    "                                replace=False,\n",
    "                                n_samples=len(subset_1),\n",
    "                                random_state=42)\n",
    "\n",
    "df = pd.concat([subset_0_downsampled, subset_1])\n",
    "\n",
    "print(\"Down Sampling-Done.\")\n",
    "\n",
    "X = df.drop([\"risk_flag\"], axis=1)\n",
    "y = df[\"risk_flag\"].apply(lambda x: int(x))\n",
    "\n",
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"Scaling-Done.\")\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\", max_iter=100000, C=0.1) # max_iter: default -1\n",
    "\n",
    "# Start: training\n",
    "start_time = time.time()\n",
    "print(\"- Start: training\")\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# End: training\n",
    "end_time = time.time()\n",
    "print(\"- End: training\\n\")\n",
    "\n",
    "# Calculate training time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "y_pred = clf.predict(X_val_scaled)\n",
    "\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(f\"AUC = {auc:.3f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time // 60} min {elapsed_time % 60:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9b58f-c739-4dd2-9d97-011886300283",
   "metadata": {},
   "source": [
    "# SVM with PySpark\n",
    "- Pyspark는 원격에서 호스팅되는 .csv파일을 읽을 수 없음 --> 로컬 파일로 돌려야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67ed2870-ab3b-46c7-9aa8-6d27dc6c0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col as scol\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eea51965-6626-46a0-939e-95267962c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding-Done.\n",
      "Down Sampling-Done.\n",
      "Scaling-Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.499\n",
      "Elapsed Time: 0.0 min 2.70 sec\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"UNLV\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"Id\").withColumnRenamed(\"married/single\", \"married_single\")\n",
    "df = df.toDF(*(col.lower() for col in df.columns))\n",
    "df = df.withColumn(\"risk_flag\", scol(\"risk_flag\").cast(\"integer\"))\n",
    "\n",
    "# Category cols to num\n",
    "cate_cols = [\"married_single\", \"profession\", \"house_ownership\", \"car_ownership\", \"city\", \"state\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_idx\").fit(df) for col in cate_cols]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(*cate_cols)\n",
    "\n",
    "print(\"Label Encoding-Done.\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col for col in df.columns if col != \"risk_flag\"],\n",
    "    outputCol=\"origin_features\"\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "df = df.select([\"origin_features\", \"risk_flag\"])\n",
    "\n",
    "# Down sampling\n",
    "pos_count = df.filter(\"risk_flag = 1\").count()\n",
    "neg_df = df.filter(\"risk_flag = 0\")\n",
    "sampled_neg_df = neg_df.orderBy(rand(seed=42)).limit(pos_count)\n",
    "df = sampled_neg_df.union(df.filter(\"risk_flag = 1\"))\n",
    "\n",
    "print(\"Down Sampling-Done.\")\n",
    "\n",
    "# StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"origin_features\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(df)\n",
    "df_scaled = scaler_model.transform(df)\n",
    "\n",
    "df = df_scaled.select([\"features\", \"risk_flag\"])\n",
    "\n",
    "print(\"Scaling-Done.\")\n",
    "\n",
    "# Define model\n",
    "train, val = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "svm = LinearSVC(labelCol=\"risk_flag\", weightCol=\"risk_flag\", maxIter=10000) # maxIter: default 100\n",
    "\n",
    "# Start: training\n",
    "start_time = time.time()\n",
    "\n",
    "model = svm.fit(train)\n",
    "\n",
    "# End: training\n",
    "end_time = time.time()\n",
    "# Calculate training time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "predictions = model.transform(val)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"risk_flag\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC = {auc:.3f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time // 60} min {elapsed_time % 60:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1773d-d684-466a-8aa1-bd917b4fddd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dayoung",
   "language": "python",
   "name": "dayoung"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
