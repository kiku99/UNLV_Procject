# UNLV_Procject

**Definition of scaleable machine learning for big data**: Big data analytics provides scalable solutions for distributed or very large data. For this project, students will have hands-on experience installing Apache Spark, and practicing python libraries, e.g., MLLib, while applying supervised and unsupervised machine learning algorithms to large datasets for data analysis and developing machine learning models.

## Weekly plan 
	a. Week 1: Introduction to the project / Study Apache Spark / System setup
	b. Week 2: Practice Apache Spark libraries
	c. Week 3: Model training / Model validation / Experiments
	d. Week 4: Documentation / Presentation / Demo / Wrap-up

## Required skills
	 a. Understanding of MapReduce
	b. Machine learning
	c. Fundamental Python skills

## Datasets

	a. Visual Object Classes Challenge 2012 (VOC2012). Data Source: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
	b. Semantics Boundaries Dataset and Benchmark. Data source: http://home.bharathh.info/pubs/codes/SBD/download.html

## Papers

- Scalable machine-learning algorithms for big data analytics: a comprehensive review
- MLlib: Machine Learning in Apache Spark 
- PySpark : High-performance data processing without learning Scala 
- Spark: Cluster Computing with Working Sets 
- Scalable Machine Learning Using PySpark
