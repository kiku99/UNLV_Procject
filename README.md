# UNLV_Procject

**Definition of scaleable machine learning for big data**: Big data analytics provides scalable solutions for distributed or very large data. For this project, students will have hands-on experience installing Apache Spark, and practicing python libraries, e.g., MLLib, while applying supervised and unsupervised machine learning algorithms to large datasets for data analysis and developing machine learning models.

## Weekly planâ€¨
- Week 1: Introduction to the project / Study Apache Spark / System setup
- Week 2: Practice Apache Spark libraries
- Week 3: Model training / Model validation / Experiments
- Week 4: Documentation / Presentation / Demo / Wrap-up

## Required skills
- Understanding of MapReduce
- Machine learning
- Fundamental Python skills

## Datasets
- Visual Object Classes Challenge 2012 (VOC2012). Data Source: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
- Semantics Boundaries Dataset and Benchmark. Data source: http://home.bharathh.info/pubs/codes/SBD/download.html

## Papers

- [Spark: Cluster Computing with Working Sets](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)
- [PySpark : High-performance data processing without learning Scala](https://www.ibm.com/downloads/cas/DVRQZYOE)
- [MLlib: Machine Learning in Apache Spark](https://www.jmlr.org/papers/volume17/15-237/15-237.pdf)
- [Scalable Machine Learning Using PySpark](https://ieeexplore.ieee.org/document/9842696)
- [Scalable machine-learning algorithms for big data analytics: a comprehensive review](https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/widm.1194)
  
